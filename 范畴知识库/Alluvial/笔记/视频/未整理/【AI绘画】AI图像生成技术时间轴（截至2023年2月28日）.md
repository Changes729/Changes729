> 视频来源：https://www.bilibili.com/video/BV12s4y1o7ys/?spm_id_from=333.1245.0.0&vd_source=b736aa3d7f0fdf47b59ea3021dc810ab

# 【AI绘画】AI图像生成技术时间轴（截至2023年2月28日）

## 关键词

- 非监督学习
- 前馈神经网络（弗兰克罗森布拉特创造感知器，perceptron）
- 反向传播算法（backpropagation）
- 卷积神经网络（CNN）：为什么卷积神经网络是从人眼结构中寻找的灵感？
  - 人类眼睛结构
- 循环神经网络（RNN）：霍普菲尔德神经网络
- GPU上实现CNN
- 玻尔兹曼机
- 深度学习
  - 自编码器（autoencoder）
  - 深度置信网络（DBN）
- CUDA：通用计算处理器
- 变分自编码器（VAE）：一种学习压缩数据的概率潜在表示的方法，该方法由一个将数据投影到潜在空间的编码器和一个从潜在空间将点映射回原始数据分布的解码器组成
- 对抗生成网络（GAN）
  - StyleGAN—TGAN
- 注意力机制（attention）
- 扩散概率模型（DPM）：非平衡热力学
  - Langevin动力学
- Transformer模型：Transformer的主要作用是提高自然语言处理（NLP）任务的准确性和效率，通过self—attention更好地理解序列中的依赖关系，从而更好地捕捉语义信息。在NLP中，文本被分割为称为“token”的单元，然后通过为其分配独立向量来应用attention。Transformer逐渐扩散到其他领域， 如在图像领域中，Vision Transformer（ViT）已经成为主流，其原理是 将图像分割为小块并对它们进行token化。
  - 去噪扩散概率模型（DDPM）：有改进型DDPM
    - ADM（Ablated Diffusion Model）：对DDPM的U—Net进行改进
    - PNDM（扩散模型伪数值方法）
  - 去噪扩散隐式模型（DDIM）
  - 使用扩散方法的超分辨率模型SR3
  - CDM （Cascaded Diffusion Models）
  - Classifier-free guidance （CFG）
  - 渐进式蒸馏
  - GLIDE：OpenAI，text2image扩散模型
  - LDM（Latent Diffusion Models）
  - Video Diffusion Models：谷歌视频的扩散模型
  - DALL  E2：OpenAI发布
- CLIP模型
  - DALLE：OpenAI发布
  - CogView：DALLE的改进型
    - CogView2
- InvokeAI：用户图形界面（GUI）
- Stable Diffusion
  - LAION公开数据库
  - NovelAI模型
    - ACertain系列
    - Counterfeit系列
    - OrangeMixs系列
    - ChilloutMix
    - NovelAI Diffusion SMEA新的采样方法
  - AnythingV3
  - LoRA模型
    - 低秩自适应（Low—rank Adaptation）
  - ControlNet
- DiT (Diffusion Transformer)
- pix2pix-zero
- Directed Diffusion



## 摘记

**20世纪40年代～70年代**

1943年：美国神经科学家沃伦麦卡洛克和逻辑学家沃尔特皮茨提出以人类神经元为灵感的计算模型

1949年：美国心理学家唐纳德赫布提出赫布理论，为**非监督学习**创造理论基础

1958年：美国心理学家弗兰克罗森布拉特创造感知器（perceptron） ，一种最简单形式的**前馈神经网络**

1969年：苏联数学家弗拉基米尔瓦普尼克在《支持向量机》一书中时首次提出反向传播算法（backpropagation）

1975年：美国社会科学家保罗韦博斯提出将反向传播算法应用于神经网络训练



**20世纪80年代～90年代**

1980年：日本计算机科学家福岛邦彦从**人类眼睛结构获得灵感，提出用于文字识别的新认知机（neocognitron），即卷积神经网络（CNN）**

1982年：美国物理学家约翰霍普菲尔德发明**霍普菲尔德神经网络，为一种循环神经网络（RNN）**

1985年：加拿大计算机学家杰弗里辛顿提出玻尔兹曼机(Boltzmann machine)

1989～1998年：法国计算机科学家杨立昆将反向传播算法应用于CNN训练，提出识别手写数字的CNN架构LeNet

1997年：德国计算机科学家于尔根施密德胡伯和塞普霍赫赖特提出RNN架构长短期记忆（LSTM），后不断演进，用于处理序列输入



**21世纪00年代**

2004年：K.S.Oh 和 K.Jung 证明标准神经网络（ANN）可以在图形处理单元（GPU）上大大加速

2006年：K.Chellapilla等人第一次在GPU上实现CNN

2006年：杰弗里辛顿基于**玻尔兹曼机提出自编码器（autoencoder）和深度置信网络（DBN），深度学习概念初步形成**

2007年：英伟达发布CUDA，将GPU扩展为通用计算处理器(GPGPU)



**2010年～2013年**

2010年：Dan Ciresan等人证明在GPU上可以通过反向传播方法快速进行深ANN的监督学习

2012年：Alex Krizhevsky利用GPU训练的深度CNN架构AlexNet 赢得lmageNet挑战赛，证明深度神经网络（DNN）的可行性

2013年：Kingma等人提出**变分自编码器（VAE）一种学习压缩数据的概率潜在表示的方法，该方法由一个将数据投影到潜在空间的编码器和一个从潜在空间将点映射回原始数据分布的解码器组成。**



**2014年**

6月10日：Goodfellow等人提出对**抗生成网络（GAN）**

此前的主流生成模型，通过让生成器和鉴别器相互竞争来提高生成质量。由一个从潜在空间生成逼真数据的生成器和一个识别真假数据的鉴别器组成。

12月1日：Bahdanau等人提出**注意力机制（attention）**，以解 决seq2seq模型固定长度编码向量时出现的瓶颈问题

聚合在序列之间具有强关联的要素之间的信息。提取不同序列之间相关性的被称为交叉注意力机制（Cross—Attention），而提取自身与其它要素之间关系的被称为自注意力机制（Self—Attention）。



**2015年～2019年**

2015年3月12日：Dickstein等人受非平衡热力学启发，提出一种**扩散概率模型（DPM）**

Langevin动力学是对分子运动系统建模的一种方法，扩散概率模型在**逆向扩散中使用随机梯度Langevin动力学**进行有效采样。

2017年6月12日：Vaswani等人提出Transformer模型

Transformer的主要作用是提高自然语言处理（NLP）任务的准确性和效率，通过self—attention更好地理解序列中的依赖关系，从而更好地捕捉语义信息。在NLP中，文本被分割为称为“token”的单元，然后通过为其分配独立向量来应用attention。Transformer逐渐扩散到其他领域， 如在图像领域中，Vision Transformer（ViT）已经成为主流，其原理是 将图像分割为小块并对它们进行token化。



**2020年6月19日**

Ho等人提出**去噪扩散概率模型（DDPM）**

从一组随机噪声样本中学习数据分布。核心思想是应用可逆动力学过程来模拟数据的逐渐清晰化过程，从而生成高质量的样本。DDPM模型基于扩散过程，它通过一系列条件化的扩散方程模拟数据清晰化的过程，并利用神经网络来学习每个步骤中的条件概率分布。



**2020年10月6日**

Song等人提出**去噪扩散隐式模型（DDIM）**

通过随机性控制来改善速度和质量之间的平衡



**2021年1月5日**

OpenAI发布DALLE：基于CLIP模型（一个月后发布），非扩散模型。Text2image的先驱。



**2021年2月18日**

Nichol等人提出改进型DDPM：通过学习更合适的噪声强度，改善生成速度和品质之间的平衡



**2021年2月26日**

Radford等人提出将自然语言和图像在同一特征空间中相近的模块CLIP

Text2lmage领域的重大里程碑、基础模型。通过进行大规模对比预训练，将不可能严格一对一对应的不同概念连接起来。



**2021年4月15日**

Saharia等人提出了一种使用扩散方法的超分辨率模型SR3

在人脸图像和自然图像的超分辨率任务中，经人工评估超越了传统的

FSRGAN和PULSE方法，获得了与真实图像相媲美的结果。



**2021年5月11日**

Dhariwa等人提出了ADM（Ablated Diffusion Model）

对DDPM的U—Net进行改进，进一步证明了通过扩散模型实现高质量生成的可行性。在FID和多样性上一举超越StyleGAN2和BigGAN，成为当时的state-of-the-art。



**2021年5月26日**

清华大学、阿里达摩院、北京智源推出CogView：DALLE的改进型，非扩散模型。



**2021年5月30日**

Ho等人提出**CDM （Cascaded Diffusion Models）**从低分辨率起阶段性生成图像。使用低分辨率图像作为条件，训练生成更高分辨率图像的U—Net。



**2021年9月28日**

Ho等人提出**Classifier-free guidance （CFG）**不使用分类模型，训练可用于类别条件的扩散模型和非条件模型。生成时可以任意调整强度，将样本采样朝着特定标签的方向移动。



**2021年9月29日**

谷歌提出渐进式蒸馏逐渐精简模型以减少扩散模型逆扩散过程所需的步骤。



**2021年12月20日**

OpenAI发布GLIDE。第一个text2image扩散模型。



**2021年12月20日**

Rombach等人提出**LDM（Latent Diffusion Models）**提高计算效率。



**2021年12月31日**

百度推出多模态生成模型文心ERNIE—ViLG通过统一的预训练框架实现文本和图像的双向生成，非扩散模型。



**2022年1月1日**

英伟达提出EDM从一个统一的角度将扩散模型的理论和实践分开，对提高性能有许多影响。



**2022年2月20日**

浙江大学提出PNDM（扩散模型伪数值方法）由DDPM创建的流形上的伪数值求解方法，加速采样过程。



**2022年3月6日**

DiscoDiffusion在GitHub公开 开源text2image扩散模型先行者。



**2022年3月24日**

Meta AI推出Make-A-Scene试图以人工标注的分割图为训练样本，来提高图像生成的可控性。非扩散模型。



**2022年4月7日**

谷歌提出Video Diffusion Models 首个针对视频的扩散模型。



**2022年4月13日**

OpenAI推出DALL  E2基于扩散模型， text2image技术开始逐渐为大众所知。



**2022年4月28日**

清华大学推出CogView2，CogView的改进型，非扩散模型。



**2022年5月23日**

谷歌推出Imagen基于扩散模型，号称超越DALL.E2。



**2022年5月29日**

清华大学、北京思源提出CogVideo基于预训练text2image模型的text2video模型。



**2022年6月2日**

清华大学提出DPM—Solver提出基于信噪比的ODE数值解法，加速采样过程。



**2022年6月22日**

谷歌推出Parti通过自回归生成模型进一步改善text2image的参数可扩展性。非扩散模型。



**2022年7月12日**

MidJourney上线β测试成为text2image扩散模型扩圈的契机。人人都能简单地生成炫酷图片的时代到来。



**2022年8月2日**

Gal等人提出Textual Inversion通过训练额外的文本嵌入层（embedding）来对扩散模型进行个性化。

谷歌提出Prompt2Prompt通过重新编写prompt来编辑图像。



**2022年8月7日**

InvokeAI公开早期较为知名的用户图形界面（GUI）之一。相比后来的WebUI更易用，但扩展较少。



**2022年8月22日**

Stable Diffusion公开 The Big Bang.以CLIP为条件输入的LDM。使用LAION公开数据库训练。因为开源属性，用户急剧增加。

Stable Diffusion WebUI公开扩展度极高，目前已成为主流GUI。引入了负面提示词（negative prompt）的用法。

Negative prompt并非删除prompt，而是定义了降噪采样的起点，即从negative prompt提示的U—Net输出端向prompt的U—Net输出端方向延伸。后与textual inversion方法融合，形成了专用的negative embedding。



**2022年8月25日**

谷歌提出DreamBooth对扩散模型进行针对特定概念类型的微调。



**2022年8月30日**

Waifu Diffusion公开基于Stable Diffusion，利用Danbooru的数据进行finetune。此后各种基于SD的自炼模型层出不穷。



**2022年9月22日**

谷歌提出Phenaki使用在时间维度对动画进行压缩的C—ViViT，可以从文本生成任意长度的视频。



**2022年9月29日**

谷歌提出Re-Imagen基于扩散模型，通过从数据库中搜索图像来稳定生成稀有文本。

Meta AI提出Make—A—Video无需文本和视频配对的text2video模型。



**2022年10月3日**

Hong等人提出Self-Attention Guidance通过促进关注领域的精细化，提高推理时的生成质量。

NovelAI上线AI绘画功能基于Stable Diffusion，利用Danbooru带标签图片530万张进行继续训练。

其生成的动漫风格图片几乎可以以假乱真，成为热点话题。上线仅10天，生成的图片就超过了3000万张。至11月16日，生成图片数已超过1.2亿。



**2022年10月5日**

谷歌提出Imagen Video可以生成写实的、精细的、自然的视频。



**2022年10月6日**

斯坦福大学提出两步蒸馏提升无分类器指导扩散模型的采样效率。



**2022年10月6日～8日**

NovelAI模型泄露

WebUI跟进更新，导致作者AUTOMATIC1111本人在Stable Diffusion官方Discord被封禁、剥夺身份。



**2022年10月11日**

NovelAI主动公开相对Stable Diffusion的改进点

CLIP使用倒数第二层的隐藏状态，这与谷歌的Imagen相同。

Bucketing，训练时输入不同尺寸的图片，后NovelAI主动开源此技术。

超网络（Hypernetwork） ，利用单独训练的网络对扩散模型的U—Net内cross—attention层的k、 v参数进行finetune。

SD的CLIP只能接收75个token， NovelAI通过等距阶段tokens可以实现token数量3倍（后在WebUI中进一步扩展为无限长）。



**2022年10月～**

随着NovelAl模型泄露，用户数量开始爆发性增加， Al绘画逐渐普及。

普通用户对AI绘画的研究开始加快，其中的代表便是以《元素法典》为首的提示工程产物。



**2022年10月17日**

谷歌提出Imagic一种可以在保持图像一致性的情况下使用prompt进行编辑的技术。



**2022年10月20日**

Meta AI提出DiffEdit自动估计编辑所需的mask以提高编辑质量。



**2022年10月27日**

百度推出多模态生成模型文心ERNIE—ViLG 2.0基于扩散模型，大型中文text2image模型。对文本和图像的处理以及逆向扩散过程进行了调整。



**2022年10月28日**

百度提出UPainting基于扩散模型，跨模态引导的统一text2image生成模型。提出方法解决从简单prompt生成到复杂画面的统一处理问题。

字节跳动提出MagicMix提出了一种新任务类型，称语义混合（Semantic Mixing）



**2022年11月2日**

英伟达提出eDiff基于扩散模型，通过组合多个模型，实现性能的提升。

清华大学提出DPM—Solver++一种即使在有引导的情况下也能稳定、高质量生成的采样方法。



**2022年11月5日**

MidJourney v4 α测试提高细节及多人图性能。较短的prompt也可生成较高品质图片。



**2022年11月7日**

NijiJourney封闭β测试开始12月2日正式上线。

AnythingV3在Huggingface公开实际上此前已在国内流传，为贴吧吧友作品。此后在1月14日、1月15日更新了V4.0和V4.5。



**2022年11月～**

随着NovelAl泄露模型的普及，越来越多的人开始进行继续训练，还有人利用模型融合来达到更好的效果

模型具体数量以及来源已经难以统计，较为常见的有：

ACertain系列：利用Dreambooth针对动画风格进行训练。此外，还有与AnythingV3融合后形成的ACertainThing；

Counterfeit系列：同样使用Dreambooth对融合模型进行finetune的模型；

OrangeMixs系列：融合了大量模型，目前最新版为AOM3。还有与Elysium系列融合的EOM、与Anything系列融合的BOM等；

ChilloutMix：写实风格融合模型。由于肖像权争议，作者本人于2023年2月28日自主删除，后由CivitAI平台官方接管。



**2022年11月17日**

Brooks等人提出InstructPix2Pix一个响应部分编辑指令进行图像编辑的通用模型。



**2022年11月20日**

字节跳动提出MagicVideo采用LDM的轻量化text2video模型。



**2022年11月21日**

Dong等人提出DreamArtist通过仅有一张图像，实现比传统方法更清晰、更多样、更具操作性的扩散模型个性化。

Adobe推出SceneComposer基于扩散模型。可以在控制物体形状的同时，任意调整精度。



**2022年11月23日**

微软提出ReCo基于扩散模型，通过指定区域坐标来控制任意绘图对象的位置。



**2022年11月25日**

Meta AI提出SpaText基于扩散模型，通过添加分割图，控制任意绘图对象的位置和形状。



**2022年11月27日**

南洋理工大学、京东研究院提出Uni3D使用离散扩散模型，同时生成语言和图像



**2022年11月30日**

Daras等人提出Multiresolution Textual Inversion多分辨率textual inversion。



**2022年12月1日**

Park等人提出Shape-Guided Diffusion明确指定编辑区域的图像编辑。



**2022年12月7日**

GitHub用户cloneofsimo将用于NLP的LoRA引入扩散模型：低秩自适应（Low—rank Adaptation）是2021年1月17日Hu等人提出的一种用于在迁移学习任务中调整预训练模型的技术。它的主要目的是提高模型的泛化能力，以适应新的目标任务，同时减少对目标任务数据的依赖。

Stable Diffusion 2.1公开。Prompt处理改为OpenCLIP，并且采用了NovelAl一样的CLIP跳过最后一层的设置。Prompt形式改变，艺术风格和画手的性能有所提高。

Weinbach等人提出M-VADER 使用图像作为prompt。



**2022年12月8日**

卡耐基梅隆大学、清华大学、Adobe提出Custom Diffusion将多个新概念组合在一起的高自由度构图。

Zhang等人提出SINE：实现包括图像整体风格和分辨率变更在内的灵活编辑。



**2022年12月9日**

Feng等人提出StructureDiffusion 将promt分解并解离元素纠缠的方法。



**2022年12月19日**

Peebles等人提出DiT (Diffusion Transformer)

一种用transformer取代U—Net的方法，符合当下ViT的趋势。应用于LDM时，比传统方法生成质量有所提高，state—of—the—art。



**2022年12月22日**

腾讯提出Tune-A-Video利用预训练text2image模型的one—shot text2video。



**2023年1月2日**

谷歌提出Muse一种比扩散模型和自回归生成模型更高效的state—of—the—art方法。



**2023年1月3日**

Liu等人提出Composable Diffusion 可组合的扩散模型。



**2023年1月17日**

Li等人提出GLIGEN利用基于基础语言（grounded language）信息的辅助输入来控制预训练模型。



**2023年1月23日**

英伟达提出StyleGAN—TGAN卷土重来，堂堂复活。



**2023年2月6日**

Parmar等人提出pix2pix-zero无需编辑原始prompt或进行附加训练的图像编辑。



**2023年2月6日**

Runway提出Gen-1可以通过文本和图像引导，进行视频编辑。



**2023年2月10日**

Zhang等人提出ControlNet向预训练的扩散模型添加多种辅助输入路径。



**2023年2月12日**

Waifu Diffusion 1.5 β

基于Stable Diffusion 2.1。困难重重屡次延期的epoch 1测试版本，使用了独特的美学分数，以求达到更好的效果，也因此训练的样本从一开始计划的1500万张减少为1000万张。



**2023年2月14日**

Bansal等人提出Universal Guided Diffusion扩展分类器导向方法，无需重新训练即可添加辅助输入路径。



**2023年2月15日**

Adobe提出PRedltOR受到unCLIP的扩散先验分布的启发，实现了无需进行附加训练的图像编辑。



**2023年2月16日**

Tal等人提出MultiDiffusion无需重新训练即可在任意分辨率下赋予空间控制性。



**2023年2月16日**

腾讯ARC提出T21—Adapter将预训练模型中潜在的表现力转化为精细的控制能力。



**2023年2月20日**

阿里巴巴、蚂蚁集团提出Composer基于扩散模型，通过自由组合的条件约束，在end2end的方式下实现高度的操作性。



**2023年2月21日**

NovelAI Diffusion SMEA新的采样方法，改善传统采样器在更高分辨率进行采样时，产生重复人物或奇怪的解剖结构的问题。



**2023年2月22日**

Du等人提出Reduce, Reuse, Recycle基于马尔科夫链蒙特卡洛方法（MCMC）的新型采样和构造式生成的提议。



**2023年2月23日**

Gal等人提出E4T (Encoder for Tuning)基于扩散模型，可以极快、极灵活地从一张图像中获取新概念。

谷歌、UC伯克利提出基于二元奖励估计的一致性改进受ChatGPT使用的RLHF（人类反馈强化学习）方法影响，使用人工反馈校对文本与图像。



**2023年2月25日**

Ma等人提出Directed Diffusion通过使用交叉注意力图来控制物体绘图区域。


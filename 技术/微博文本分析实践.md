# 微博文本分析实践

> 最近在准备浙大工业设计工程考研，太难了。感觉需求抓不住，有的需求呢又太小，有的需求呢又太社会。所以，快题设计从需求分析开始就卡住了手脚。
>
> 没办法，遇到问题解决问题，即使考不上浙大，能学到东西就万岁吧。
>
> 这里就是在尝试通过抓数据的方式来了解人群。

## 起源

**01 - 《创新设计思维（第二版）》**

创新设计思维设计步骤中，背景理解、移情观察、主题定制、方案设计、可行性分析、行动计划与原型设计、推广与反思。可以发现，在方案设计前，有三步是需要做前期研究的。背景理解不到位、需求掌握不透彻，设计就无法灵活变通。

设计领域经典的双钻模型：**思维发散到思维收敛**，如果都没有发散过内容的话，还没到收敛的一步，再收敛就归零了，没法产出创新想法。

我现在准备快题就是这么一个情况，本来有一个想法，从需求到设计实践到后续工作，但是呢，学长给意见说：不够大、不够新、不够浙大，等等，就，Over。

说的比较抽象，希望大家能理解。

**02 - [[21设 + 清华大学 魏毅] - 设计实用工具介绍](请前往微信搜索21设，搜索第十期分享回顾——设计实用用具介绍：心理学方法论篇)** 

最近听21设公开课，清华大学的学长很热情的分享了在心理测量领域对创新设计的看法，并且提出了许多心理测量与用户研究工具。

里面呢讲到一个文献法研究：

> **文献法**：可能是过时的。
>
> - 怎么看：带有目的、找对关键词、按照正确顺序
> - 看什么：**论文（博士文章）**、**书籍**、报告、信息爬虫、音频视频
> - 信息处理方法：归纳演绎法

看到了吗？看博士论文获取关键词，看领域书籍获取知识，看报告获取前沿观点。音频视频是放最后的。个人理解，音频视频太花时间了，信息参差不齐而且非结构化。这里面唯一我没有用过的方法就是**信息爬虫**。主要是自己不知道信息爬虫有什么用。

**03 - [[一席 - 茅明睿] - 数据与城市](https://yixi.tv/#/speech/detail?id=145)**

然后呢，在云栖大会上听到了茅明睿的演讲，然后里面有个例子很让我激动——他们通过微博文本分析的方法来获取用户的需求。而且提供了一个观点，就是用户在网络上的诉求和现实中的诉求是相一致的，如果我们可以通过网络“感知”到用户，那么对网络用户进行设计，同样可以满足线下的用户的需求。



## 关键词定位 与 文献学习

然后我就，开始搜关键字啊。视频里能搜集到的关键字有：语料库、文本聚类。

然后有了以下学习过程：

- 一开始在Google搜语料库。没有了解到什么。

- 然后在知网研学上搜语料库博士论文。了解到我不明白语料库，还是回归文本聚类。

- 在Google上搜文本聚类。太泛泛了，感觉需要案例学习。

- 在CSDN上搜微博文本聚类Demo。太专业了，CSDN上的博客复制粘贴严重，大部分都没有数据、也没有成套代码、算法讲解因为是复制粘贴，也不够清晰。我想还是先获取到微博数据吧。

- 在CSDN上搜微博爬虫。一堆自己实现的爬虫——这效率能高吗？

- 在Google上搜微博爬虫，找Github开源的。最终找到了：https://github.com/dataabc/weibo-search 。蛮好，能用。抓了许多，这里就放一个吧：[自行车道 关键字抓包](/api/file/capture/weibo/自行车道.csv)，如果觉得数据量太大，这里有个小的[杭州交通 关键字抓包](/api/file/capture/weibo/杭州交通.csv) 。然后呢，就需要对文本进行分析，得到一些具有代表性的文本内容，就和视频里显示的效果那样。

- 还是在CSDN上搜微博爬虫

  [[CSDN] - 短评论聚类并显示提取属性](https://blog.csdn.net/zy4321234zx/article/details/89841890?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163498812816780269889723%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163498812816780269889723&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-89841890.pc_search_ecpm_flag&utm_term=%E8%AF%84%E8%AE%BA%E8%81%9A%E7%B1%BB&spm=1018.2226.3001.4187)

  [[知乎] - word2vec是如何得到词向量的？](https://www.zhihu.com/question/44832436)

  [[知乎] - 互联网上的语料库](https://www.zhihu.com/question/21177095)

  [[CSDN] - 使用python对微博评论进行分词、文本聚类](https://blog.csdn.net/weixin_43873702/article/details/111931428?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link)

  都可以看一看。了解一下。

- 最后使用这篇[[CSDN] - python进行中文文本聚类（切词以及Kmeans聚类）](https://blog.csdn.net/yyxyyx10/article/details/63685382?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.no_search_link&spm=1001.2101.3001.4242.0)开始写代码了。网上有两种方法进行文本聚类的，一种是`jieba+sklearn`，另一种是用`tensorflow`的。找文章一会儿找`sklearn`的，一会儿又遇到`tensorflow`好像能出效果，花了巨多时间。



## 实现

对中文的文本相似性聚类，需要以下步骤：

1. 获取数据
2. Python读入数据
3. 数据清洗
4. 分词
5. 停用词去除（有些文章说停用词不用去掉，这里因为原理理解不清晰，所以还是去掉先）
6. 计算TF（Term Frequency）就是一个词在句子中出现的次数
   1. 将分好词的文本矩阵的所有分词整合成一个大的词典（dictionary）
   2. 针对每一个文本，计算其在词典中单个分词出现的频率（出现次数除以总量）
7. 计算IDF（Inverse Data Frequency）
   1. 计算每一个出现的分词在所有文本中出现的频率`log（总文本数/出现次数）`
8. 将TF与IDF相乘得到TF-IDF矩阵
9. 通过Kmeans对矩阵进行聚类



## 其他

[THUCTC](http://thuctc.thunlp.org/):由清华大学自然语言处理实验室推出的中文文本分类工具包

[[CSDN] - 用Python对自己的文章做文本分析](https://blog.csdn.net/lgzlgz3102/article/details/106644203?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163497322616780269899555%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163497322616780269899555&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-106644203.pc_search_ecpm_flag&utm_term=python+%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90&spm=1018.2226.3001.4187)

> TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。**字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降**。（引自：关键字提取算法TF-IDF和TextRank（python3））


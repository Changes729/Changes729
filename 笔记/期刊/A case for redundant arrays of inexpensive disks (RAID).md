> 原文链接：https://dl.acm.org/doi/pdf/10.1145/971701.50214
>
> 作者：David A. Patterson, Garth Gibson, and Randy H. Katz
>
> 参考资料：
>
> - [[OSTEP] RAID](https://pages.cs.wisc.edu/~remzi/OSTEP/Chinese/38.pdf)：某大学教材，还有中文版

# A case for redundant arrays of inexpensive disks (RAID)

**摘要.** 如果 I/O 的性能提升相不能与CPU和内存的性能提升相匹配，那这种性能提升就会造成浪费。虽然单个 <u>SLED (Single Large Expensice Disk, 大型昂贵磁盘)</u> 的容量快速增长，但 SLED 的性能提升却很有限。<u>RAID (Redundant Arrays of Inexpensive, 廉价磁盘冗余阵列)</u> 基于为个人计算机开发的磁盘技术，相较于 SLED 提供了一种有吸引力的替代方案，有望在<u>性能（performance）、可靠性（reliability）、功耗（power consumption）和可扩展性（scalability）</u>方面实现一个数量级的改进。本文介绍了 RAID 的五个级别，给出了它们的相对成本/性能，并将 RAID 与 IBM 3380 和 Fujitsu Super Eagle 进行了比较。



## 1. 背景：CPU与内存的性能提升

目前，计算机用户正享受着计算机速度空前的增长。 Gordon Bell 表示，1974 年至 1984 年间，单片机的性能每年提高 40%，大约是小型机速度的两倍 [Bell 84]。接下来的一年，比尔·乔伊 (Bill Joy) 预测增长速度会更快 [Joy 85]：

$$MIPS = 2^{Year - 1984}$$

大型机和超级计算机制造商难以跟上 "Joy's Law" 预测的快速增长步伐，因此通过提供多处理器作为其顶级产品来应对。



但快速的 CPU 并不意味着快速的系统。 Gene Amdahl 使用此规则将 CPU 速度与主内存大小相关联 [Siewiorek 82]：

*每秒每条CPU指令需要一个字节的主存；*

如果计算机系统成本不由内存成本主导，那么阿姆达尔常数表明内存芯片容量应该以相同的速度增长。戈登·摩尔 (Gordon Moore) 在 20 多年前就预测了这一增长率：

$$三极管/芯片 = 2^{Year - 1964}$$

正如摩尔定律所预测的那样，RAM 的容量每两年 [Moore 75] 到三年 [Myers 86] 就会翻两番。



最近，主内存兆字节（Megabytes）与 MIPS 的比率被定义为 alpha [Garcia 84]，阿姆达尔常数的含义 alpha = 1。部分原因是内存价格的快速下降，主内存大小的增长速度快于 CPU 速度，并且许多今天发货的机器的 alpha 值为 3 或更高。



为了保持计算机系统的成本平衡，辅助存储必须与系统其他部分的进步相匹配。磁盘技术的增长关键是每平方英寸可存储的最大位数的增长，或者是磁道中每英寸位数的增长每英寸磁道数。称为 M.A.D.，对于最大面密度，“盘密度第一定律”预测 [Frank87]：

$$MAD = 10^{(Year - 1971) / 10}$$

磁盘技术的容量每三年翻一番，价格减半，与半导体存储器的增长速度一致，实际上，在 1967 年至 1979 年间，IBM 数据处理系统的平均磁盘容量超过了其主存储器 [Stevens81] 。



容量并不是唯一必须快速增长以维持系统平衡的内存特性，因为指令和数据传送到 CPU 的速度也决定了其最终性能。主存的速度之所以能够保持同步，有两个原因：

1.  高速缓存的发明，表明可以自动管理一个小缓冲区来包含大部分内存引用； 
2. 以及SRAM技术，用于构建缓存，其速度以每年40%到100%的速度提高。

与主内存技术相比，SLED 的性能以普通的速度提高。这些机械设备主要受寻道和旋转延迟的影响：从 1971 年到 1981 年，高端 IBM 磁盘的原始寻道时间仅提高了两倍，而旋转时间没有变化[Harker81]，更大的密度意味着找到信息时传输速率更高，额外的磁头可以减少平均寻道时间，但原始寻道时间仅以每年 7% 的速度改善。没有理由期望在不久的将来会出现更快的速度。



为了保持平衡，计算机系统一直在使用更大的主存储器或固态磁盘来缓冲一些 I/O 活动。对于 I/O 活动具有引用局部性并且波动性不是问题的应用程序来说，这可能是一个很好的解决方案，但是对于小数据块（例如事务处理）的高随机请求率或由少量的海量数据请求（例如在超级计算机上运行的大型模拟）面临着严重的性能限制。



## 2. 未决的 I/O 危机

提高问题某些部分的性能，同时保持其他问题不变，会产生什么影响？阿姆达尔的答案现在被称为阿姆达尔定律 [Amdahl67]：

$$S = \frac{1}{(1-f) + f/k}$$

其中：

$S = $ 有效加速

$f = $ 快速模式下的工作部分

$k = $ 在快速模式下加速

假设当前某些应用程序将 10% 的时间花费在 I/O 上。然后，当计算机速度提高 10 倍时（根据 Bill Joy 的说法，在短短三年内），阿姆达尔定律预测有效加速将仅为 5 倍。当我们的计算机速度提高 100 倍时（通过单处理器或多处理器的发展），该应用程序的速度将不到 10 倍，浪费了 90% 的潜在加速。

虽然我们可以想象通过缓冲近期 I/O 需求来改进软件文件系统，但我们需要创新来避免 I/O 危机 [Boral 83]。



## 3. 解决方案：廉价磁盘阵列（AID）

大磁盘容量的快速提高并不是磁盘设计者的唯一目标，因为个人计算机已经为廉价磁盘创造了市场。这些成本较低的磁盘性能较低且容量较小。下面的表一比较了顶级 IBM 3380 型号 AK4 大型机磁盘、Fujitsu M2361A“Super Eagle”小型计算机磁盘和 Conner Peripherals CP 3100 个人计算机磁盘。

一个令人惊讶的事实是，廉价磁盘中每个执行器每秒的 I/O 数量在大型磁盘的两倍之内。在其余几个指标中，包括每兆字节的价格，廉价磁盘优于或等于大型磁盘。

小尺寸和低功耗更令人印象深刻，因为 CP3100 等磁盘包含全磁道缓冲区和传统大型机控制器的大部分功能。由于标准委员会在定义更高级别的外围接口方面所做的努力，例如 ANSI X3.131-1986 小型计算机系统接口 (SCSI)，小型磁盘制造商可以在大容量磁盘中提供此类功能。此类标准鼓励 Adeptec 等公司以单芯片形式提供 SCSI 接口，从而允许磁盘公司以低成本嵌入大型机控制器功能。图 1 比较了传统大型机磁盘方法和小型计算机磁盘方法。嵌入每个磁盘中作为控制器的同一 SCSI 接口芯片也可以用作 SCSI 总线另一端的直接内存访问 (DMA) 设备。

这些特性导致我们建议将 I/O 系统构建为廉价磁盘阵列，或者交错用于超级计算机的大型传输 [Kim 86][Livny 87][Salem86]，或者独立用于事务处理的许多小型传输。根据表 I 中的信息，75 个廉价磁盘的 I/O 带宽可能是 IBM 3380 的 12 倍，并且容量相同，而且功耗和成本更低。



## 4. 注意事项

我们无法在本文的篇幅内探讨与此类阵列相关的所有问题，因此我们将重点放在性价比和可靠性的基本估计上。我们的理由是，如果在性价比方面没有优势，或者在可靠性方面没有可怕的劣势，那么就没有必要进一步探索。我们描述事务处理工作负载的特征是为了评估廉价磁盘集合的性能，但请记住，这样的集合只是完整事务处理系统的一个硬件组件。虽然基于这些想法设计完整的 TPS 很诱人，但我们将在本文中抵制这种诱惑。布线和封装当然是许多廉价磁盘阵列的成本和可靠性问题，也超出了本文的范围。



## 5. 现在是坏消息：可靠性

磁盘的不可靠性迫使计算机系统管理员在发生故障时必须频繁地备份信息。磁盘数量增加一百倍会对可靠性产生什么影响？假设故障率恒定（即，故障发生时间呈指数分布），并且故障是独立的——磁盘制造商在 MTTF (Mean Time To Failure，计算平均故障时间) 时做出的这两个假设——磁盘阵列的可靠性为：

$$磁盘阵列的平均故障时间 = \frac{单磁盘的平均故障时间}{阵列中的磁盘总数}$$

根据表 I 中的信息，100 个 CP 3100 磁盘的 MTTF 为 30,000/100 = 300 小时，即不到 2 周。与 IBM 3380 的 30,000 小时（> 3 年）MTTF 相比，这是令人沮丧的。如果我们考虑将阵列扩展到 1000 个磁盘，那么 MTTF 为 30 小时或大约 1 天，需要一个比 dismal 还要糟糕的形容词。

如果没有容错能力，大型廉价磁盘阵列就太不可靠而无法发挥作用。



## 6. 更好的解决方案：RAID

为了克服可靠性挑战，我们必须利用包含冗余信息的额外磁盘来在磁盘发生故障时恢复原始信息。这些廉价磁盘冗余阵列的缩写是 RAID。为了简化对我们最终建议的解释并避免与以前的工作混淆，我们对五种不同的磁盘阵列组织进行了分类，从镜像磁盘开始，逐步发展到具有不同性能和可靠性的各种替代方案。我们将每个组织称为 RAID 级别。

应该预先警告读者，我们将所有级别描述为在硬件中实现，只是为了简化演示，因为 RAID 思想既适用于软件实现，也适用于硬件。

**可靠性（Reliability）**：我们的基本方法是将阵列分成可靠性组，每个组都有额外的“检查”磁盘，其中包含冗余信息。当磁盘发生故障时，我们假设在短时间内故障磁盘将被替换，并且信息将使用冗余信息重建到新磁盘上。 这个时间称为**平均修复时间（mean time to repair，MTTR）**。如果系统包含额外的磁盘作为“热”备用备件，则可以缩短 MTTR；当磁盘出现故障时，将以电子方式切换替换磁盘。人工操作员会定期更换所有故障磁盘。以下是我们使用的其他术语：

$$D=有数据的磁盘总数（不包括额外的校验盘）$$(Disk)

$$G=一组数据盘数量（不包括额外的校验盘）$$(GroupDisk)

$$C = 一组中检查盘的数量；$$(Counts)

$$^nG = D/G = 组数$$

如上所述，我们做出了与磁盘制造商相同的假设——故障是指数级的且独立的。 （地震或电涌是磁盘阵列可能不会独立发生故障的情况。）由于这些可靠性预测非常高，因此我们要强调的是，可靠性仅适用于具有此故障模型的磁盘头组件，而不是整个软件和电子系统。此外，我们认为，技术的进步意味着极高的 MTTF 是“大材小用”，因为无论预期寿命如何，用户都会更换过时的磁盘。毕竟，有多少人还在使用 20 年前的磁盘？

修复一个错误 RAID 的一般 MTTF 计算分两步给出。首先，组 MTTF 为：

$$MTTF_{Group} = \frac{MTTF_{Disk}}{G + C}*\frac{1}{修复过程中发生另一个磁盘错误的概率}$$

正如附录中更正式的推导，在第一次故障修复之前发生第二次故障的概率为：

$$修复过程中发生另一个磁盘错误的概率 = \frac{MTTR}{MTTF_{Disk/ (No. Disks - 1)}}  = \frac{MTTR}{MTTF_{Disk}/(G+C-1)}$$

附录中正式计算背后的直觉来自于尝试计算 X 个单磁盘故障修复时间内第二个磁盘故障的平均数量。由于我们假设磁盘故障以统一的速率发生，因此在 X 次首次故障的修复时间内第二次故障的平均数量为：

$$\frac{X * MTTR}{组中剩余磁盘的MTTF}$$

则单个磁盘的平均第二次故障次数为：

$$\frac{X * MTTR}{MTTF_{Disk} / 组中剩余磁盘数}$$

剩余磁盘的 MTTF 就是单个磁盘的 MTTF 除以组中良好磁盘的数量，得到上面的结果。

第二步是整个系统的可靠性，大约为（因为 MTTFGroup 并不完全呈指数分布）：

$$MTTF_{RAID} = \frac{MTTF_{Group}}{^nG}$$

把它们放在一起，我们得到：

$$MTTF_{RAID} = \frac{MTTF_{Disk}}{G + C}*\frac{MTTF_{Disk}}{(G+C-1)*MTTR}*\frac{1}{^nG}$$

$$=\frac{(MTTF_{Disk})^2}{(G+C)*^nG*(G+C-1)*MTTR}$$

$$MTTF_{RAID} = \frac{(MTTF_{Disk})^2}{(D+C*^nG)*(G+C-1)*MTTR}$$

由于每个级别的公式都相同，因此我们酌情使用以下参数来具体化抽象数字：D=100 个数据盘总数，G=每组 10 个数据盘，MTTFDisk = 30,000 小时，MTTR = 1 小时，并检查盘C的磁盘数由每个组由 RAID 级别确定。

**间接可靠性成本（Reliability Overhead Cost）**。这只是额外的检查磁盘，以数据磁盘 D 数量的百分比表示。正如我们将在下面看到的，成本随着 RAID 级别的不同而变化，从 100% 到 4%。

**可用存储容量百分比（Useable Storage Capacity Percentage）**。表达这种可靠性开销的另一种方式是用可用于存储数据的数据磁盘和检查磁盘总容量的百分比来表示。根据组织的不同，该比例从最低的 50% 到最高的 96% 不等。 

**性能（Performance）**。由于超级计算机应用程序和事务处理系统具有不同的访问模式和速率，因此我们需要不同的指标来评估两者。对于超级计算机，我们计算大数据块每秒的读写次数，大数据块的定义是从一组数据磁盘中至少获取一个扇区。在大型传输期间，组中的所有磁盘充当单个单元，每个磁盘并行读取或写入大数据块的一部分。



事务处理系统的更好衡量标准是每秒单独读取或写入的数量。由于事务处理系统（例如借记/贷记）使用磁盘访问的读取-修改-写入序列，因此我们也包含该指标。理想情况下，在小型传输期间，组中的每个磁盘都可以独立操作，读取或写入独立信息。总之，超级计算机应用程序需要高数据速率，而事务处理需要高 I/O 速率。

对于大型和小型传输计算，我们假设最小用户请求是一个扇区，扇区相对于磁道较小，并且有足够的工作来保持每个设备繁忙。因此，扇区大小会影响磁盘存储效率和传输大小。图 2 显示了 RAID 中大型和小型磁盘访问的理想操作。

性能指标是大型（分组）或小型（单独）传输的每秒读取、写入和读取-修改-写入次数。我们不是给出每个指标的绝对数字，而是计算效率：RAID 每秒的事件数相对于单个磁盘每秒的相应事件数。 （这是 Boral 的每 GB I/O 带宽 [Bora] 83] 缩放到每个磁盘的 GB 数。）在本文中，我们追求的是根本差异，因此我们使用简单的、确定性的吞吐量测量来衡量我们的性能指标，而不是延迟。



**每个磁盘的有效性能（Effective Performance Per Disk）**。磁盘的成本可能占数据库系统成本的很大一部分，因此每个磁盘的 I/O 性能（考虑到检查磁盘的开销）表明了系统的成本/性能。这是 RAID 的底线。



## 7. 第一级 RAID：镜像磁盘

镜像磁盘是提高磁盘可靠性的传统方法。这是我们考虑的最昂贵的选项，因为所有磁盘都是重复的（G=1和C=1），并且对数据磁盘的每次写入也是对检查磁盘的写入。 Tandem 使容错控制器的数量增加了一倍，从而允许镜像磁盘的优化版本允许并行读取。表 II 显示了假设进行此优化的 1 级 RAID 的指标。

| MTTF                                       | 超过产品有效寿命              |
| ------------------------------------------ | ----------------------------- |
|                                            | （4,500,000 小时或 > 500 年） |
| 磁盘总数                                   | 2D                            |
| 可靠性成本（Overhead Cost）                | 100%                          |
| 可用存储百分比（Useable Storage Capacity） | 50%                           |

| Events/Sec vs. Single Disk   | Full RAID | Efficiency Per Disk |
| ---------------------------- | --------- | ------------------- |
| Large (or Grouped)Reads      | 2D/S      | 1.00/S              |
| Large (or Grouped) Writes    | D/S       | .50/S               |
| Large (or Grouped) R-M-W     | 4D/3S     | .67/S               |
| Small (or Individual) Reads  | 2D        | 1.00                |
| Small (or Individual) Writes | D         | .50                 |
| Small (or Individual) R-M-W  | 4D/3      | .67                 |

*1 级 RAID 的特征。这里我们假设写入不会因等待第二次写入完成而减慢，因为与写入整组 10 到 25 个磁盘的减慢相比，写入 2 个磁盘的减慢较小。与具有软件不可见的额外磁盘的“纯”镜像方案不同，我们假设一个优化方案，其控制器数量是两倍，允许对所有磁盘进行并行读取，为大量读取提供完整的磁盘带宽，并允许读取-修改-写入的读取并行发生。*

当单个访问分布在多个磁盘上时，平均排队、查找和旋转延迟可能与单个磁盘情况不同。尽管带宽可能不变，但它分布得更均匀，减少了排队延迟的方差，如果磁盘负载不太高，还可以通过并行性减少预期的排队延迟[Livny 87]。当许多臂寻道到同一磁道然后旋转到所描述的扇区时，平均寻道和旋转时间将大于单个磁盘的平均值，趋于最坏情况时间。这种影响通常不应超过单个扇区平均访问时间的两倍，同时仍能并行获取多个扇区。在具有足够控制器的镜像磁盘的特殊情况下，在可以读取任何数据扇区的臂之间进行选择将使平均读取寻道时间减少高达 45% [Bitton 88]。

为了考虑到这些因素，但为了保持我们的基本重点，当一组中有两个以上磁盘时，我们应用减速因子 S。一般来说，每当磁盘组并行工作时，1 < S < 2。对于同步磁盘，组中所有磁盘的主轴都是同步的，因此一组磁盘的相应扇区同时通过磁头下方，[Kurzweil 88]因此对于同步磁盘来说，没有减速并且 S=1。由于 1 级 RAID 在其组中只有一个数据磁盘，因此我们假设大型传输需要与更高级别 RAID 组中相同数量的磁盘协同工作：10 到 25 个磁盘。

复制所有磁盘可能意味着数据库系统的成本加倍或仅使用 50% 的磁盘存储容量。如此慷慨的举动激发了 RAID 的新水平。



## 8. 第二级 RAID：ECC 汉明码

主存储器组织的历史提出了一种降低可靠性成本的方法。随着 4K 和 16K DRAM 的推出，计算机设计人员发现这些新设备容易因 alpha 值而丢失信息。**由于系统中有许多单比特 DRAM，并且通常一次以 16 至 64 个芯片为一组进行访问，因此系统设计人员添加了冗余芯片来纠正单个错误并检测每组中的双错误。**这使得存储芯片的数量增加了 12% 到 38%（具体取决于组的大小），但它显着提高了可靠性。只要一起读取或写入一组中的所有数据位，就不会影响性能。但是，读取小于组大小的数据需要读取整个组以确保信息正确，而写入组的一部分意味着三个步骤：

1. 读取步骤以获取所有其余数据； 
2. 合并新旧信息的修改步骤；
3. 写入步骤写入整个组，包括检查信息。

由于 RAID 中有大量磁盘，并且某些访问是针对磁盘组的，因此我们可以通过在一组磁盘上对数据进行位交错来模仿 DRAM 解决方案，然后添加足够的检查磁盘来检测和纠正单个错误。**单个奇偶校验磁盘可以检测单个错误，但要纠正错误，我们需要足够的检查磁盘来识别有错误的磁盘。对于 10 个数据磁盘 (G) 的组大小，我们总共需要 4 个检查磁盘 (C)**，如果 G = 25，则 C = 5 [HammingSO]。为了降低冗余成本，我们假设组大小从 10 到 25 不等。由于我们的单个数据传输单元只是一个扇区，位交错磁盘意味着该 RAID 的大量传输必须至少为 G 个扇区。与 DRAM 一样，较小量的读取意味着从一组中的每个位交错磁盘读取完整扇区，而单个单元的写入涉及对所有磁盘的读取-修改-写入周期。表 III 显示了该 2 级 RAID 的指标。

| MTTF                                       | 超过产品有效寿命           |                           |
| ------------------------------------------ | -------------------------- | ------------------------- |
|                                            | G=I0                       | G=25                      |
|                                            | (494,500 hrs or >50 years) | (103,500 hrs or 12 years) |
| 磁盘总数                                   | 1.40D                      | 1.20D                     |
| 可靠性成本（Overhead Cost）                | 40%                        | 20%                       |
| 可用存储百分比（Useable Storage Capacity） | 71%                        | 83%                       |

对于大型写入，级别 2 系统具有与级别 1 相同的性能，尽管它使用较少的检查磁盘，因此在每个磁盘的基础上，它的性能优于级别 1。对于小数据传输，无论是整个系统还是每个系统的性能都很差。磁盘;一个小传输必须访问一个组的所有磁盘，限制了同时访问 DIG 的最大数量。我们还包括减速因子 S，因为访问必须等待所有磁盘完成。

因此，2 级 RAID 适合超级计算机，**但不适合事务处理系统**，因为组大小的增加会增加两个应用程序每个磁盘的性能差异。认识到这一事实，Thinking Machines Incorporated 今年为其 Connection Machine 超级计算机宣布了 2 级 RAID，称为“Data Vault”，其中 G = 32 和 C = 8，包括一个热备用备用设备 [Hillis 87].

在改进之前较小数据传输方面，我们再次专注于降低成本。



## 9. 第三级 RAID：每组单个检查磁盘

2 级 RAID 中的大多数检查磁盘用于确定哪个磁盘发生故障，因为只需要一个冗余奇偶校验磁盘即可检测到错误。这些额外的磁盘确实是“冗余的”，因为大多数磁盘控制器已经可以检测磁盘是否出现故障：通过磁盘接口中提供的特殊信号或用于检测和纠正软错误的扇区末尾的额外检查信息。因此，可以通过计算剩余好磁盘的奇偶校验，然后逐位与原始全组计算的奇偶校验进行比较，来重建故障磁盘的信息。当这两个奇偶校验一致时，失败位为 0；否则为1。如果chock磁盘出现故障，则读取所有数据磁盘并将组奇偶校验存储在替换磁盘中。对于这里考虑的组大小，将检查盘减少到每组一个(C＝1)将开销成本降低到4％到10％之间。**第三级 RAID 系统的性能与二级 RAID 相同，但每个磁盘的有效性能有所提高，因为它需要更少的检查磁盘。**

磁盘总数的减少也提高了可靠性，但由于它仍然大于磁盘的使用寿命，因此这是一个小问题。与第 3 级系统相比，第 2 级系统的一个优点是不需要与每个扇区关联的额外检查信息来纠正软错误，从而将每个磁盘的容量增加大约 10%。 **2 级还允许“即时”纠正所有软错误，而无需重新读取扇区。**表 IV 总结了第三级 RAID 特性，图 3 比较了级别 2 和级别 3 的扇区布局和检查磁盘。

| MTTF                                       | 超过产品有效寿命          |                          |
| ------------------------------------------ | ------------------------- | ------------------------ |
|                                            | G=I0                      | G=25                     |
|                                            | (820000 hrs or >90 years) | (360000 hrs or 40 years) |
| 磁盘总数                                   | 1.10D                     | 1.04D                    |
| 可靠性成本（Overhead Cost）                | 10%                       | 4%                       |
| 可用存储百分比（Useable Storage Capacity） | 91%                       | 96%                      |

Park 和 Balasubramanian 提出了第三级 RAID 系统，但没有提出特定的应用程序 [Park86]。我们的计算表明，与事务处理系统相比，它更适合超级计算机应用程序。今年，两家磁盘制造商宣布了针对此类应用的 3 级 RAID，使用 G=4 和 C=1 的同步 5.25 英寸磁盘：一个来自 Maxtor，一个来自 Micropolis [Maginnis 87]。第三个级别将可靠性开销成本降至最低水平，因此在最后两个级别中，我们在不改变成本或可靠性的情况下提高了小型访问的性能。 



## 10. 第四级 RAID：独立读/写

将传输分散到组内的所有磁盘具有以下优点： 

- 由于可以利用整个阵列的传输带宽，因此可以减少大型或分组传输时间。但它也有以下缺点： 
- 读/写一个组中的一个磁盘需要读/写一个组中的所有磁盘。 2 级和 3 级 RAID 每组一次只能执行一个 I/O。 
- 如果磁盘不同步，您将看不到平均寻道和旋转延迟。观察到的延迟应该朝着最坏的情况发展，因此上面的方程中存在 S 因子。

第四级 RAID 通过并行性（每组一次执行多个 I/O 的能力）提高了小型传输的性能。我们不再将单独的传输信息分布在多个磁盘上，而是将每个单独的单元保留在单个磁盘中。

位交织的优点是可以轻松计算检测或纠正第 2 级错误所需的汉明码。但请记住，在第三级 RAID 中，我们依靠磁盘控制器来检测单个磁盘扇区内的错误。因此，如果我们将单个传输单元存储在单个扇区中，我们可以检测单个读取的错误，而无需访问任何其他磁盘。图 3 显示了 RAID 级别 2、3 和 4 的扇区中信息存储的不同方式。通过将整个传输单元存储在扇区中，读取可以是独立的，并且可以以磁盘的最大速率进行操作，但仍能检测到错误。因此，**级别 3 和级别 4 之间的主要变化是 wc 在扇区级别而不是位级别在磁盘之间交错数据**。

乍一看，您可能认为对单个扇区的单独写入仍然涉及一组中的所有磁盘，因为 (1) 必须用新的奇偶校验数据重写校验磁盘，并且 (2) 其余数据磁盘必须被写入。读取以能够计算新的奇偶校验数据。回想一下，每个奇偶校验位只是一组中所有相应数据位的一个异或。在4级RAID中，与3级不同，奇偶校验计算要简单得多，因为如果我们知道旧数据值和旧奇偶校验值以及新数据值，我们可以计算新奇偶校验信息如下：

$$新奇偶校验= (旧数据 xor 新数据 )\ xor\ 旧分区 $$

在第 4 级中，小量写入会使用 2 个磁盘执行 4 次访问（2 次读取和 2 次写入），而小量读取仅涉及在一个磁盘上进行一次读取。表V总结了第四级RAID特性。请注意，**所有小型访问的读取速度都显着提高，但小型读取-修改-写入相对于 1 级 RAID 仍然很慢，以至于其对事务处理的适用性值得怀疑。**最近 Salem 和 Garcia-Molina 提出了 4 级系统 [Salem 86]。在进入下一个级别之前，我们需要解释表 V 中小型写入的性能（以及小型读取-修改-写入，因为它们在此 RAID 中需要相同的操作）。小写入的公式将 D 除以 2 而不是 4，因为 2 访问可以并行进行：可以同时读取旧数据和旧奇偶校验，并且可以同时写入新数据和新奇偶校验。小写的性能也除以G，因为组中的单个校验盘必须与该组中的每个小写一起读写，从而将一次可以执行的写入数量限制为组的数量。校验盘是瓶颈，最终级别的RAID消除了这个瓶颈。



## 11. 第五级 RAID：无单校验盘

虽然 4 级 RAID 实现了读取并行性，但写入仍限于每组一次，因为每次写入都必须读写检查磁盘。最后一级 RAID 将数据和校验信息分布在所有磁盘（包括校验磁盘）上。图4比较了4级和5级RAID的磁盘扇区中检查信息的位置。这个小变化对性能的影响很大，**因为 RAID 级别 5 可以支持每组多个单独的写入。**例如，假设在上面的图 4 中，我们要写入磁盘 2 的扇区 0 和磁盘 3 的扇区 1。如左图 4 所示，在 RAID 级别 4 中，这些写入必须是连续的，因为磁盘的扇区 0 和扇区 1必须写5。然而，如右图所示，在 RAID 级别 5 中，写入可以并行进行，因为对磁盘 2 的扇区 0 的写入仍然涉及对磁盘 5 的写入，但对磁盘 3 的扇区 1 的写入涉及对磁盘 4 的写入。更改使 RAID 级别 5 接近两全其美：小型读取-修改-写入现在的执行速度接近级别 1 RAID 的每个磁盘速度，同时保持每个磁盘的大型传输性能和 RAID 级别 3 的高可用存储容量百分比4. 将数据分布在所有磁盘上甚至可以提高小型读取的性能，因为每组多一个包含数据的磁盘。表 VI 总结了该 RAID 的特性。请记住前面给出的警告，如果您只想执行超级计算机应用程序，或者在存储容量有限时只想执行事务处理，或者如果您想要同时执行超级计算机应用程序和事务处理，则 5 级 RAID 显得非常有吸引力。



## 12. 讨论

在结束本文之前，我们希望指出有关 RAID 的一些更有趣的点。第一个是，虽然磁盘条带化和奇偶校验支持的方案看起来好像是由硬件完成的，但实际上没有必要这样做。我们只是给出方法，硬件和软件解决方案之间的决定严格来说是成本和效益之一。例如，在磁盘缓冲有效的情况下，5级小写操作不会有额外的磁盘读取，因为旧数据和旧奇偶校验将位于主内存中，因此软件将提供最佳性能和最低成本。在本文中，我们假设传输单位是扇区的倍数。随着最小传输单元的大小变得大于一在长时间断电的情况下备份主存储器。这些磁盘的容量较小，在重建过程中占用的数据库也较少，从而提高了可用性。 （请注意，发生故障时，级别 5 将所有磁盘捆绑在一个组中，而级别 1 在重建过程中只需要单个镜像磁盘，从而使级别 1 在可用性方面具有优势）。



## 13. 总结

RAID 提供了一种经济高效的选择，可以应对处理器和内存速度呈指数增长的挑战。我们相信个人计算机磁盘尺寸的减小是磁盘阵列成功的关键，正如 Gordon Bell 认为微处理器尺寸减小是多处理器成功的关键一样 [Bell 85]。在这两种情况下，较小的尺寸简化了许多组件以及封装和布线的互连。虽然大型机处理器（或 SLED）的大型阵列是可能的，但用相同数量的微处理器（或 PC 驱动器）构建阵列肯定更容易。正如贝尔创造了术语“多”来区分由微处理器制成的多处理器一样，我们使用术语“RAID”来标识由个人计算机磁盘制成的磁盘阵列。凭借性价比、可靠性、功耗和模块化增长方面的优势，我们预计 RAID 将在未来的 I/O 系统中取代 SLED。然而，有几个悬而未决的问题可能会影响 RAID 的实用性： 

- RAID 对延迟有何影响？ 
- 单个磁盘的非指数故障假设对 MTTF 计算有何影响？ 
- RAID 的实际寿命与使用独立故障模型计算的 MTTF 相比是多少？ 
- 同步磁盘将如何影响4 级和5 级RAID 性能？ 
- “减速”S 实际表现如何？ [Livny 87] 
- 缺陷扇区如何影响 RAID？ 
- 如何将I/O 调度到5 级RAID 以最大化写入并行性？ 
- 事务处理中是否存在磁盘访问引用的局部性？ 
- 信息能否自动重新分布到100 到1000 个磁盘上以减少争用？ 
- 磁盘控制器设计会限制RAID 性能吗？ 
- 应如何构建 100 到 1000 个磁盘并将其物理连接到处理器？ 
- 布线对成本、性能和可靠性有何影响？ 
- RAID 应该连接到CPU 的哪里才不会限制性能？内存总线？HO总线？缓存？ 
- 文件系统是否允许对不同文件采用不同的条带化策略？ 
- 固态硬盘和WORM 在RAID 中的作用是什么？ 
- “并行访问”磁盘（并行访问读/写磁头下的每个表面）对 RAID 有什么影响？



## 参考文献

[Bell 84] C.G. Bell, "The Mini and Micro Industries," IEEE Computer, Vol. 17, No. 10 (October 1984), pp. 14-30.

[Joy 85] B. Joy, presentation at ISSCC '85 panel session, Feb. 1985. 

[Siewiorek 82] D.P. Siewiorek, C.G. Bell, and A. Newell, Computer Structures: Principles and Examples, p. 46. 

[Moore 75] G.E. Moore, "Progress in Digital Integrated Electronics," Proc. IEEE Digital Integrated Electronic Device Meeting, (1975), p. 11. 

[Myers 86] G.J. Myers, A.Y.C. Yu, and D.L. House, "Microprocessor Technology Trends," Proc. IEEE, Vol. 74, no. 12, (December 1986), pp. 1605-1622. 

[Garcia 84] H. Garcia-Molina, R. Cullingford, P. Honeyman, R. Lipton, "The Case for Massive Memory," Technical Report 326, Dept, of EE and CS, Princeton Univ., May 1984. 

[Myers 86] W. Myers, "The Competitiveness of the United States Disk Industry," IEEE Computer, Vol. 19, No. 11 (January 1986), pp. 85-90. 

[Frank 87] P.D. Frank, "Advances in Head Technology," presentation at Challenges in Disk Technology Short Course, Institute for Information Storage Technology, Santa Clara University, Santa Clara, California, December 15-17, 1987. 

[Stevens 81] L.D. Stevens, "The Evolution of Magnetic Storage," IBM Journal ofResearch and Development, Vol. 25, No. 5, Sept. 1981, pp. ESS-615.   

[Harker81] J.M. Harker et al., "A Quarter Century of Disk File Innovation," ibid., pp. 677-689.   

[Amdahl 67] G.M. Amdahl, "Validity of the single processor approach to achieving large scale computing capabilities," ProceedingsAFIPS1967 Spring Joint Computer Conference Vol. 30 (Atlantic City, New Jersey April 1967), pp. 483-485. 

[Boral 83] H. Boral and D.J. DeWitt, "Database Machines: An Ideas Whose Time Has Passed? A Critique of the Future of Database Machines," Proc. International Corf, on Database Machines, Edited by H.-O. Leilich and M. Misskoff, Springer-Verlag, Berlin, 1983. 

[IBM 87] "IBM 3380 Direct Access Storage Introduction," IBM GC 26-4491-0, September 1987. 

[Gawlick 87] D. Gawlick, private communication, Nov., 1987. 

[Fujitsu 87] "M2361A Mini-Disk Drive Engineering Specifications," (revised) Feb., 1987, B03P-4825-0001A. 

[Adaptec 87] AIC-6250, IC Product Guide, Adaptec, stock # DB0003-00 rev. B, 1987, p. 46. 

[Livny87] Livny, M., S. Khoshafian, H. Boral, "Multi-disk management algorithms," Proc. of ACM SIGMETRICS, May 1987. 

[Kim 86] M.Y. Kim, "Synchronized disk interleaving," IEEE Trans, on Computers, vol. C-35, no. 11, Nov. 1986. 

[Salem 86] K. Salem and Garcia-Molina, H., "Disk Striping," IEEE 1986 Ini. Conf. on Data Engineering, 1986. 

[Bitton 88] D. Bitton and J. Gray, "Disk Shadowing," in press, 1988. 

[Kurzweil 88] F. Kurzweil, "Small Disk Arrays - The Emerging Approach to High Performance," presentation at Spring COMPCON 88, March 1, 1988, San Francisco, CA. 

[Hamming 50] R. W. Hamming, "Error Detecting and Correcting Codes," The Bell System Technical Journal, Vol XXVI, No. 2 (April 1950), pp. 147-160. 

[Hillis 87] D. Hillis, private communication, October, 1987. 

[Park 86] A. Park and K. Balasubramanian, "Providing Fault Tolerance in Parallel Secondary Storage Systems," Department of Computer Science, Princeton University, CS-TR-057-86, Nov. 7, 1986. 

[Maginnis 87] N.B. Maginnis, "Store More, Spend Less: Mid-range Options Abound."Computerworld, Nov. 16,1987, p. 71. 

[Denning 78] P.J. Denning and D.R. Slutz, "Generalized Working Sets for Segment Reference Strings," CACM, vol. 21, no. 9, (Sept. 1978) pp. 750-759. 

[Bell 85] Bell, C.G., "Multis: a new class of multiprocessor computers,"Science, vol. 228 (April 26, 1985) 462-467.
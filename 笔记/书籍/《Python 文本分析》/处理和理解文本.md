# 处理和理解文本

## 文本分析框架

- NLTK：自然语言工具包是一个包含50多个语料库和词汇资源的完整平台
- pattern：它为Web挖掘、信息检索、NLP（Natural language processing，自然语言处理）、机器学习和网络分析提供了工具和接口
- gensim：具有一套丰富的语义分析功能，包括主题建模和相似性分析。而其最重要的是包含一个谷歌非常流行的word2vec模型
- textblob：它是一个提供多种功能的库，包括文本处理、短语提取、分类、POS标注、文本翻译和情感分析
- spacy：通过每种技术和算法的最佳实现来提供工业级NLP功能。



## 处理和理解文本

所有的机器学习算法，无论是有监督的还是无监督的，通常都会使用数值格式的输入特征。虽然这是特征工程下单一个独立主体，但是我们仍然将详细地探讨它。为了实现数值格式的特征输入，你需要清洗、规范化和预处理初始文本数据。通常，文本语料库和原始文本的数据格式即非准确也非规范的。



## 文本切分

**01 句子切分（sentence tokenization）**

- `sent_tokenize`
- `PunktSentenceTokenizer`
- `RegexpTokenizer`

```python
# 基于默认框架进行切分
default_st = nltk.sent_tokenize
alice_sentences = default_st(text=alice)

# 基于语料库训练的切分器进行切分
german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')
german_sentences = german_tokenizer.tokenize(german_text)

# 通过正则表达式进行切分
SENTENCE_TOKENS_PATTERN = r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<![A-Z]\.)(?<=\.|\?|\!)\s'
regex_st = nltk.tokenize.RegexpTokenizer(
            pattern=SENTENCE_TOKENS_PATTERN,
            gaps=True)
sample_sentences = regex_st.tokenize(sample_text)
```



**02 词语切分（word tokenization）**

- `word_tokenize`
- `TreebankWordTokenizer`
- `RegexpTokenizer`

```python
# 基于默认词语切分器
default_wt = nltk.word_tokenize
words = default_wt(sample_text)

# 基于TreebankWordTokenizer
treebank_wt = nltk.TreebankWordTokenizer()
words = treebank_wt.tokenize(sample_text)

# 基于RegexpTokenizer
TOKEN_PATTERN = r'\w+'        
regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,
                                gaps=False)
words = regex_wt.tokenize(sample_text)
```



**03 文本规范化**

- 文本清洗：比如 nltk 中的 `clean_html()` 函数用来清除 HTML 结构数据
- 文本切分
- 删除特殊字符
- 拓展缩写词：将 `is not` 缩写为 `isn't`
- 大小写转换
- 删除停用词：指一些没有，或者只有极小意义的词语
- 词语矫正：指一些拼写错误，还有英文中情绪化的表达
- 词干提取：英文中，通过在词干上添加词缀来创建新词，这个过程反过来就是词干提取
- 词形还原：和词干提取比较类似



**04 理解文本句法和结构**

- 词性（POS）标签
- 浅层分析
- 基于依存关系的解析
- 基于成分结构的解析

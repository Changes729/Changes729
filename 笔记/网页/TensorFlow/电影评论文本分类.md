> 网页链接：https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/keras/text_classification.ipynb?hl=zh-cn#scrollTo=ItXfxkxvosLH
>
> 参考链接
>
> - [What does GlobalAveragePooling1D do in keras?](https://stackoverflow.com/questions/75067335/what-does-globalaveragepooling1d-do-in-keras)

# 电影评论文本分类

## 关键词

- 二元分类器：正面、负面

- [Large Movie Review Dataset](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fai.stanford.edu%2F~amaas%2Fdata%2Fsentiment%2F)：其中包含了 [Internet Movie Database](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.imdb.com%2F) 中的 50,000 条电影评论文本 。

- [TextVectorization layer](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/#textvectorization-layer)：文本矢量化，一个预处理层，将文本特征映射到整数序列。归属于 [Preprocessing layers](https://keras.io/api/layers/preprocessing_layers/#preprocessing-layers)

  - **max_tokens**：该层词汇表的最大大小。

- [Embedding layer](https://keras.io/api/layers/core_layers/embedding/#embedding-layer)：将索引转换为固定大小的密集向量。属于[Core layers](https://keras.io/api/layers/core_layers/)

  - 稀疏向量（Sparse Vectors）：就是全向量、完整向量，里面可能会有很多的0。
  - 密集向量（Dense Vectors）：就是把0去掉后密集表示的向量。

- [GlobalAveragePooling1D](https://keras.io/api/layers/pooling_layers/global_average_pooling1d/)：时态数据的全局平均池化操作。平均值降维。

- [BinaryCrossentropy](https://keras.io/api/losses/probabilistic_losses/#binarycrossentropy-class)：计算真实标签和预测标签之间的交叉熵损失。

- [BinaryAccuracy](https://keras.io/api/metrics/accuracy_metrics/#binaryaccuracy-class)：计算预测与二进制标签匹配的频率。0-1比较。同时会把一些浮点数直接通过阈值二分成0-1

- [sigmoid function](https://keras.io/api/layers/activations/#sigmoid-function)：`sigmoid(x) = 1 / (1 + exp(-x))`

  ![Sigmoid Function](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24547f85f71e3bd2339f8_pasted%20image%200%20(5).jpg)



## 摘记

您将使用 `text_dataset_from_directory` 实用工具创建带标签的 `tf.data.Dataset`。

```python
batch_size = 32
seed = 42

raw_train_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)
```



您将使用有用的 `tf.keras.layers.TextVectorization` 层对数据进行标准化、词例化和向量化。

```python
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation),
                                  '')
max_features = 10000
sequence_length = 250

vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)

# Make a text-only dataset (without labels), then call adapt
train_text = raw_train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text)

def vectorize_text(text, label):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text), label

# retrieve a batch (of 32 reviews and labels) from the dataset
text_batch, label_batch = next(iter(raw_train_ds))
first_review, first_label = text_batch[0], label_batch[0]
print("Review", first_review)
print("Label", raw_train_ds.class_names[first_label])
print("Vectorized review", vectorize_text(first_review, first_label))
```



```python
acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

plt.show()
```

虚线代表训练损失和准确率，实线代表验证损失和准确率。


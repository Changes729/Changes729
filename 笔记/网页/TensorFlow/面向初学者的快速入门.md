> 网页链接：[面向初学者的快速入门](https://www.tensorflow.org/tutorials/quickstart/beginner?hl=zh-cn)
>
> 参考资料：
>
> - [Keras - MNIST](https://keras.io/api/datasets/mnist/#load_data-function)：MNIST 数据集
> - [How does the Flatten layer work in Keras?](https://stackoverflow.com/questions/44176982/how-does-the-flatten-layer-work-in-keras)：展示了 Flatten 层的工作过程
> - [Dropout in Neural Networks](https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9)：解释了什么是 Dropout

# 面向初学者的快速入门

## 关键词

-  [MNIST 数据集](http://yann.lecun.com/exdb/mnist/)：是个数字，28*28的图片数据集，灰度图

  ```python
  mnist = tf.keras.datasets.mnist
  
  (x_train, y_train), (x_test, y_test) = mnist.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0
  
  print(x_train.shape, y_train.shape)
  ```

- [Sequential model](https://keras.io/guides/sequential_model/)：顺序模型，每一层只有一个输入和一个输出。

  - [Flatten layer](https://keras.io/api/layers/reshaping_layers/flatten/)：展平层。把高维数据变成一维数据。附属在 [Reshaping Layers](https://keras.io/api/layers/reshaping_layers/)
  - [Dense layer](https://keras.io/api/layers/core_layers/dense/)：常规的链接层，附属在 [Core layers](https://keras.io/api/layers/core_layers/)
    - `units`：正整数，输出空间的维数。
    - `activation`：逐个元素激活函数，如果不指定，则使用线性激活 `a(x) = x`
    - `kernel`：权重矩阵 
    - `bias`：是由图层创建的偏置向量
    - `Input Tensor`：`(batch_size, ..., input_dim)`
    - `Output Tensor`： `(batch_size, ..., units)`
  - [Dropout layer](https://keras.io/api/layers/regularization_layers/dropout/)：丢弃层，此层会随机丢弃一些输入的值，并且将剩下的一些值按 `1/(1-rate)` 的比例放大。它是用来防止过拟合（overfitting）的。附属在 [Regularization layers](https://keras.io/api/layers/regularization_layers/)
    - `rate`：丢弃的输入单位比例

- [激活函数（Layer activation functions）](https://keras.io/api/layers/activations/)：

  - `relu/ReLU` （Rectified Linear Unit）

    ![undefined](https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/ReLU_and_GELU.svg/1920px-ReLU_and_GELU.svg.png)

- 损失函数：实际于预测差距越大（或者是根本无法预测的话），损失函数的值越大。

  - [SparseCategoricalCrossentropy](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)：附属于 [Probabilistic losses](https://keras.io/api/losses/probabilistic_losses/#probabilistic-losses)

- Optimizers：优化器算法

  - [Adam](https://keras.io/api/optimizers/adam/#adam)：一种梯度下降算法。

- metrics：模型性能度量指标

  - [Accuracy](https://keras.io/api/metrics/accuracy_metrics/#accuracy-metrics)：准确度指标

- Tensors：N Dimensional arrays

- NN：neural Network



## 摘记

In the overfitting problem, <u>the model learns the statistical noise</u>. To be precise, <u>the main motive of training is to decrease the loss function</u>, given all the units (neurons). 

模型会学习统计噪声，因此，训练的主要动机是减少损失函数。



构建 `tf.keras.Sequential` 模型：

```python
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])
```

对于每个样本，模型都会返回一个包含 logits 或 log-odds 分数的向量，每个类一个。

```python
predictions = model(x_train[:1]).numpy()
predictions
```

```sh
array([[-0.31651554, -0.2934535 ,  0.5151348 ,  0.05881967,  0.28640798,
         0.30166027, -0.0038079 ,  0.42565835, -0.4255188 , -0.42769873]],
      dtype=float32)
```

`tf.nn.softmax` 函数将这些 logits 转换为每个类的*概率*：

```python
tf.nn.softmax(predictions).numpy()
```

```sh
array([[0.06794387, 0.069529  , 0.15607432, 0.09889089, 0.12416427,
        0.12607259, 0.09288754, 0.14271587, 0.06092714, 0.06079447]],
      dtype=float32)
```

使用 `losses.SparseCategoricalCrossentropy` 定义训练的损失函数：

```python
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
loss_fn(y_train[:1], predictions).numpy()
```

```sh
2.0708976
```

损失函数采用真实值向量和逻辑向量，并返回每个样本的标量损失。此损失等于真实类的负对数概率：如果模型确定类正确，则损失为零。

这个未经训练的模型给出的概率接近随机（每个类为 1/10），因此初始损失应该接近 `-tf.math.log(1/10) ~= 2.3`。



## 训练并评估模型

使用 `Model.fit` 方法调整您的模型参数并最小化损失：

```python
model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test,  y_test, verbose=2)
probability_model = tf.keras.Sequential([
  model,
  tf.keras.layers.Softmax()
])
probability_model(x_test[:1])
```





